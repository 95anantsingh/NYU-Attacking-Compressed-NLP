{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-15 03:29:25.647957: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"\n",
    "import os.path\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "# from allennlp.data.dataset_readers.stanford_sentiment_tree_bank import \\\n",
    "#     StanfordSentimentTreeBankDatasetReader\n",
    "from reader_new import StanfordSentimentTreeBankDatasetReader_NEW\n",
    "from allennlp.data.data_loaders import SimpleDataLoader\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.models import Model\n",
    "from allennlp.modules.seq2vec_encoders import PytorchSeq2VecWrapper\n",
    "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders.embedding import _read_pretrained_embeddings_file\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "from allennlp.nn.util import get_text_field_mask\n",
    "from allennlp.training.metrics import CategoricalAccuracy\n",
    "from allennlp.training.trainer import Trainer\n",
    "from allennlp.common.util import lazy_groups_of\n",
    "from allennlp.data.token_indexers import SingleIdTokenIndexer\n",
    "from allennlp.nn.util import move_to_device\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmClassifier(Model):\n",
    "    def __init__(self, word_embeddings, encoder, vocab):\n",
    "        super().__init__(vocab)\n",
    "        self.word_embeddings = word_embeddings\n",
    "        self.encoder = encoder\n",
    "        self.linear = torch.nn.Linear(in_features=encoder.get_output_dim(),\n",
    "                                      out_features=vocab.get_vocab_size('labels'))\n",
    "        self.accuracy = CategoricalAccuracy()\n",
    "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, tokens, label):\n",
    "        mask = get_text_field_mask(tokens)\n",
    "        embeddings = self.word_embeddings(tokens)\n",
    "        encoder_out = self.encoder(embeddings, mask)\n",
    "        logits = self.linear(encoder_out)\n",
    "        output = {\"logits\": logits}\n",
    "        if label is not None:\n",
    "            self.accuracy(logits, label)\n",
    "            output[\"loss\"] = self.loss_function(logits, label)\n",
    "        return output\n",
    "\n",
    "    def get_metrics(self, reset=False):\n",
    "        return {'accuracy': self.accuracy.get_metric(reset)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the binary SST dataset.\n",
    "single_id_indexer = SingleIdTokenIndexer(lowercase_tokens=True) # word tokenizer\n",
    "\n",
    "# use_subtrees gives us a bit of extra data by breaking down each example into sub sentences.\n",
    "reader = StanfordSentimentTreeBankDatasetReader_NEW(granularity=\"2-class\",\n",
    "                                                token_indexers={\"tokens\": single_id_indexer},\n",
    "                                                use_subtrees=True)\n",
    "\n",
    "train_data = reader.read('./data/train.txt')\n",
    "reader = StanfordSentimentTreeBankDatasetReader_NEW(granularity=\"2-class\",\n",
    "                                                token_indexers={\"tokens\": single_id_indexer})\n",
    "dev_data = reader.read('./data/dev.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_path = \"./lstm_main_sst_model/w2v_\" + \"vocab\"\n",
    "vocab = Vocabulary.from_files(vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24c9749109104fcb99dbf9c2e361bf1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1999995 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embedding_path = \"./data/crawl-300d-2M.vec.zip\"\n",
    "weight = _read_pretrained_embeddings_file(embedding_path,\n",
    "                                          embedding_dim=300,\n",
    "                                          vocab=vocab,\n",
    "                                          namespace=\"tokens\")\n",
    "token_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n",
    "                            embedding_dim=300,\n",
    "                            weight=weight,\n",
    "                            trainable=False)\n",
    "word_embedding_dim = 300\n",
    "word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})\n",
    "\n",
    "\n",
    "encoder = PytorchSeq2VecWrapper(torch.nn.LSTM(word_embedding_dim,\n",
    "                                              hidden_size=512,\n",
    "                                              num_layers=2,\n",
    "                                              batch_first=True))\n",
    "# word_embeddings2 = BasicTextFieldEmbedder({\"tokens\": token_embedding})\n",
    "# encoder2 = PytorchSeq2VecWrapper(torch.nn.LSTM(word_embedding_dim,\n",
    "#                                               hidden_size=512,\n",
    "#                                               num_layers=2,\n",
    "#                                               batch_first=True))\n",
    "\n",
    "# student_model = LstmClassifier(word_embeddings2, encoder2, vocab)\n",
    "teacher_model = LstmClassifier(word_embeddings, encoder, vocab)\n",
    "model_path = \"./lstm_main_sst_model/w2v_model.th\"\n",
    "    \n",
    "with open(model_path, 'rb') as f:\n",
    "    teacher_model.load_state_dict(torch.load(f,map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SimpleDataLoader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_dl \u001b[38;5;241m=\u001b[39m \u001b[43mSimpleDataLoader\u001b[49m(\u001b[38;5;28mlist\u001b[39m(train_data), batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m train_dl\u001b[38;5;241m.\u001b[39mindex_with(vocab)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SimpleDataLoader' is not defined"
     ]
    }
   ],
   "source": [
    "train_dl = SimpleDataLoader(list(train_data), batch_size=128, shuffle=True)\n",
    "train_dl.index_with(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dl = SimpleDataLoader(list(dev_data), batch_size=128, shuffle=True)\n",
    "val_dl.index_with(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "student_model.train().cuda()\n",
    "teacher_model.train().cuda()\n",
    "optimizer = optim.Adam(student_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn_kd(outputs, labels, teacher_outputs):\n",
    "    params = {\n",
    "            \"alpha\": 0.95,\n",
    "            \"temperature\": 6,\n",
    "    }\n",
    "    alpha = params['alpha']\n",
    "    T = params['temperature']\n",
    "    KD_loss = nn.KLDivLoss()(F.log_softmax(outputs/T, dim=1),\n",
    "                             F.softmax(teacher_outputs/T, dim=1)) * (alpha * T * T) + \\\n",
    "              F.cross_entropy(outputs, labels) * (1. - alpha)\n",
    "    return KD_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages/torch/nn/functional.py:2886: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8520642201834863\n",
      "0.8509174311926605\n",
      "0.8646788990825688\n"
     ]
    }
   ],
   "source": [
    "for ep in range(3):\n",
    "    student_model.train()\n",
    "    for k in train_dl:\n",
    "        k = move_to_device(k, device=0)\n",
    "        toks = k['tokens']\n",
    "        labs = k['label']\n",
    "        output_teacher = teacher_model(toks, labs)\n",
    "        output_student = student_model(toks, labs)\n",
    "        loss = (loss_fn_kd(output_student['logits'].view(-1, 2), labs.view(-1), output_teacher['logits']))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    student_model.eval()\n",
    "    student_model.get_metrics(reset=True)\n",
    "    for k in val_dl:\n",
    "        k = move_to_device(k, device=0)\n",
    "        toks = k['tokens']\n",
    "        labs = k['label']\n",
    "        output_student = student_model(toks, labs)\n",
    "    print(student_model.get_metrics()['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'lstm_distilled_sst_model/w2v_model.th'\n",
    "with open(model_path, 'wb') as f:\n",
    "    torch.save(student_model.state_dict(), f)\n",
    "# vocab.save_to_files(vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swapenv",
   "language": "python",
   "name": "swap_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
