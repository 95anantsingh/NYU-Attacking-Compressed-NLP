{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|##########| 1999995/1999995 [00:25<00:00, 79008.42it/s]\n",
      "model:  fp32  \t Size (MB): 34.727044\n",
      "model:  int8  \t Size (MB): 23.44499\n",
      "1.48 times smaller\n",
      "/scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages/allennlp/modules/encoder_base.py:99: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  num_valid = torch.sum(mask[:, 0]).int().item()\n",
      "/scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages/allennlp/nn/util.py:173: TracerWarning: Using len to get tensor shape might cause the trace to be incorrect. Recommended usage would be tensor.shape[0]. Passing a tensor of different shape might lead to errors or silently give incorrect results.\n",
      "  index_range = torch.arange(0, len(sequence_lengths), device=sequence_lengths.device)\n",
      "/scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages/allennlp/modules/encoder_base.py:112: TracerWarning: Converting a tensor to a Python list might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  sorted_sequence_lengths[:num_valid].data.tolist(),\n",
      "/scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages/allennlp/modules/encoder_base.py:110: UserWarning: pack_padded_sequence has been called with a Python list of sequence lengths. The tracer cannot track the data flow of Python values, and it will treat them as constants, likely rendering the trace incorrect for any other combination of lengths.\n",
      "  packed_sequence_input = pack_padded_sequence(\n",
      "/scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages/allennlp/modules/seq2vec_encoders/pytorch_seq2vec_wrapper.py:89: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if num_valid < batch_size:\n",
      "/scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages/allennlp/training/metrics/categorical_accuracy.py:60: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if (gold_labels >= num_classes).any():\n",
      "/scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages/allennlp/training/metrics/categorical_accuracy.py:97: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  _total_count = torch.tensor(gold_labels.numel())\n",
      "/scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages/allennlp/training/metrics/categorical_accuracy.py:100: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  self.correct_count += dist_reduce_sum(_correct_count).item()\n",
      "/scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages/allennlp/training/metrics/categorical_accuracy.py:101: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  self.total_count += dist_reduce_sum(_total_count).item()\n"
     ]
    }
   ],
   "source": [
    "! python quantization.py -q \"8\"\n",
    "# ! python quantization.py -q \"16\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|##########| 1999995/1999995 [00:25<00:00, 79238.90it/s]\n",
      "LstmClassifier(\n",
      "  (word_embeddings): BasicTextFieldEmbedder(\n",
      "    (token_embedder_tokens): Embedding()\n",
      "  )\n",
      "  (encoder): PytorchSeq2VecWrapper(\n",
      "    (_module): LSTM(300, 512, num_layers=2, batch_first=True)\n",
      "  )\n",
      "  (linear): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\n",
      "/scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1008: UserWarning: Using non-full backward hooks on a Module that does not take as input a single Tensor or a tuple of Tensors is deprecated and will be removed in future versions. This hook will be missing some of the grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using non-full backward hooks on a Module that does not take as input a \"\n",
      "Without Triggers: 0.8941441441441441\n",
      "Current Triggers: to, the, are,  : 0.8806306306306306\n",
      "Current Triggers: sabotaged, cutting-room, ill-constructed,  : 0.05855855855855856\n",
      "Current Triggers: sabotaged, cutting-room, ill-constructed,  : 0.05855855855855856\n",
      "Current Triggers: worthless, ill-conceived, ill-constructed,  : 0.03153153153153153\n",
      "Current Triggers: worthless, ill-conceived, ill-constructed,  : 0.03153153153153153\n",
      "Current Triggers: crap, ill-conceived, faulty,  : 0.03153153153153153\n",
      "Current Triggers: crap, ill-conceived, faulty,  : 0.03153153153153153\n",
      "Current Triggers: crap, subplots, faulty,  : 0.024774774774774775\n",
      "Current Triggers: an, a, in,  : 0.8918918918918919\n",
      "Current Triggers: uselessly, cutting-room, cutting-room,  : 0.05855855855855856\n",
      "Current Triggers: uselessly, cutting-room, cutting-room,  : 0.05855855855855856\n",
      "Current Triggers: stinks, shoddy, knock-off,  : 0.038288288288288286\n",
      "Current Triggers: stinks, shoddy, knock-off,  : 0.038288288288288286\n",
      "Current Triggers: moron, shoddy, substandard,  : 0.02702702702702703\n",
      "Current Triggers: moron, shoddy, substandard,  : 0.02702702702702703\n",
      "Current Triggers: moron, idiots, ill-constructed,  : 0.02252252252252252\n",
      "Current Triggers: with, the, to,  : 0.8851351351351351\n",
      "Current Triggers: cutting-room, mopping, poorly-constructed,  : 0.06981981981981981\n",
      "Current Triggers: cutting-room, mopping, poorly-constructed,  : 0.06981981981981981\n",
      "Current Triggers: barf, retro-refitting, substandard,  : 0.02252252252252252\n",
      "Current Triggers: barf, retro-refitting, substandard,  : 0.02252252252252252\n",
      "Current Triggers: stench, retro-refitting, faulty,  : 0.02027027027027027\n",
      "Current Triggers: stench, retro-refitting, faulty,  : 0.02027027027027027\n",
      "Current Triggers: turd, retro-refitting, faulty,  : 0.02252252252252252\n",
      "Current Triggers: so, to, the,  : 0.8783783783783784\n",
      "Current Triggers: miserably, cutting-room, cutting-room,  : 0.05855855855855856\n",
      "Current Triggers: miserably, cutting-room, cutting-room,  : 0.05855855855855856\n",
      "Current Triggers: worthless, mctiernan, ill-constructed,  : 0.02252252252252252\n",
      "Current Triggers: worthless, mctiernan, ill-constructed,  : 0.02252252252252252\n",
      "Current Triggers: stench, mctiernan, faulty,  : 0.015765765765765764\n",
      "Current Triggers: stench, mctiernan, faulty,  : 0.015765765765765764\n",
      "Current Triggers: stench, mctiernan, faulty,  : 0.015765765765765764\n",
      "Current Triggers: an, to, or,  : 0.8400900900900901\n",
      "Current Triggers: uselessly, ransacked, ill-constructed,  : 0.033783783783783786\n",
      "Current Triggers: uselessly, ransacked, ill-constructed,  : 0.033783783783783786\n",
      "Current Triggers: sucks, ransacked, faulty,  : 0.024774774774774775\n",
      "Current Triggers: sucks, ransacked, faulty,  : 0.024774774774774775\n",
      "Current Triggers: sucks, fizzability, faulty,  : 0.024774774774774775\n",
      "Current Triggers: sucks, fizzability, faulty,  : 0.024774774774774775\n",
      "Current Triggers: toolbags, fizzability, faulty,  : 0.024774774774774775\n",
      "Current Triggers: a, are, the,  : 0.8828828828828829\n",
      "Current Triggers: uselessly, ill-constructed, sabotaged,  : 0.06756756756756757\n",
      "Current Triggers: uselessly, ill-constructed, sabotaged,  : 0.06756756756756757\n",
      "Current Triggers: 10-course, ill-constructed, faulty,  : 0.04054054054054054\n",
      "Current Triggers: 10-course, ill-constructed, faulty,  : 0.04054054054054054\n",
      "Current Triggers: moron, waterlogged, faulty,  : 0.024774774774774775\n",
      "Current Triggers: moron, waterlogged, faulty,  : 0.024774774774774775\n",
      "Current Triggers: moron, fanatics, faulty,  : 0.02252252252252252\n",
      "Current Triggers: the, so, an,  : 0.8873873873873874\n",
      "Current Triggers: sabotaged, ill-constructed, ill-constructed,  : 0.05630630630630631\n",
      "Current Triggers: sabotaged, ill-constructed, ill-constructed,  : 0.05630630630630631\n",
      "Current Triggers: stupidity, ill-constructed, ripoff,  : 0.02927927927927928\n",
      "Current Triggers: stupidity, ill-constructed, ripoff,  : 0.02927927927927928\n",
      "Current Triggers: idiots, ill-constructed, faulty,  : 0.02702702702702703\n",
      "Current Triggers: idiots, ill-constructed, faulty,  : 0.02702702702702703\n",
      "Current Triggers: idiots, ruined, faulty,  : 0.02702702702702703\n",
      "Current Triggers: so, or, a,  : 0.8130630630630631\n",
      "Current Triggers: miserably, substandard, retread,  : 0.06981981981981981\n",
      "Current Triggers: miserably, substandard, retread,  : 0.06981981981981981\n",
      "Current Triggers: toolbags, substandard, kahlories,  : 0.04504504504504504\n",
      "Current Triggers: toolbags, substandard, kahlories,  : 0.04504504504504504\n",
      "Current Triggers: moron, shoddy, substandard,  : 0.02702702702702703\n",
      "Current Triggers: moron, shoddy, substandard,  : 0.02702702702702703\n",
      "Current Triggers: moron, idiots, faulty,  : 0.024774774774774775\n",
      "Current Triggers: to, are, a,  : 0.8896396396396397\n",
      "Current Triggers: sabotaged, ill-constructed, cutting-room,  : 0.05855855855855856\n",
      "Current Triggers: sabotaged, ill-constructed, cutting-room,  : 0.05855855855855856\n",
      "Current Triggers: stupidity, ill-constructed, faulty,  : 0.02927927927927928\n",
      "Current Triggers: stupidity, ill-constructed, faulty,  : 0.02927927927927928\n",
      "Current Triggers: idiots, ruined, faulty,  : 0.02702702702702703\n",
      "Current Triggers: idiots, ruined, faulty,  : 0.02702702702702703\n",
      "Current Triggers: sucks, ruined, faulty,  : 0.024774774774774775\n",
      "Current Triggers: in, a, are,  : 0.8851351351351351\n",
      "Current Triggers: cutting-room, cutting-room, poorly-constructed,  : 0.06531531531531531\n",
      "Current Triggers: cutting-room, cutting-room, poorly-constructed,  : 0.06531531531531531\n",
      "Current Triggers: worthless, fanatics, substandard,  : 0.024774774774774775\n",
      "Current Triggers: worthless, fanatics, substandard,  : 0.024774774774774775\n",
      "Current Triggers: moron, fanatics, faulty,  : 0.02252252252252252\n",
      "Current Triggers: moron, fanatics, faulty,  : 0.02252252252252252\n",
      "Current Triggers: moron, fanatics, faulty,  : 0.02252252252252252\n"
     ]
    }
   ],
   "source": [
    "! python create_sst_uat2.py -t \"1\" -v \"main\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|##########| 1999995/1999995 [00:25<00:00, 78805.54it/s]\n",
      "LstmClassifier(\n",
      "  (word_embeddings): BasicTextFieldEmbedder(\n",
      "    (token_embedder_tokens): Embedding()\n",
      "  )\n",
      "  (encoder): PytorchSeq2VecWrapper(\n",
      "    (_module): LSTM(300, 512, num_layers=2, batch_first=True)\n",
      "  )\n",
      "  (linear): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\n",
      "/scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1008: UserWarning: Using non-full backward hooks on a Module that does not take as input a single Tensor or a tuple of Tensors is deprecated and will be removed in future versions. This hook will be missing some of the grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using non-full backward hooks on a Module that does not take as input a \"\n",
      "Without Triggers: 0.8271028037383178\n",
      "Current Triggers: so, a, are,  : 0.8177570093457944\n",
      "Current Triggers: captivates, captivating, captivating,  : 0.10747663551401869\n",
      "Current Triggers: captivates, captivating, captivating,  : 0.10747663551401869\n",
      "Current Triggers: deeply, captivated, captivated,  : 0.08177570093457943\n",
      "Current Triggers: deeply, captivated, captivated,  : 0.08177570093457943\n",
      "Current Triggers: deeply, captivated, enhances,  : 0.07710280373831775\n",
      "Current Triggers: deeply, captivated, enhances,  : 0.07710280373831775\n",
      "Current Triggers: deeply, captivated, enhances,  : 0.07710280373831775\n",
      "Current Triggers: so, the, a,  : 0.8247663551401869\n",
      "Current Triggers: captivates, captivating, captivating,  : 0.10747663551401869\n",
      "Current Triggers: captivates, captivating, captivating,  : 0.10747663551401869\n",
      "Current Triggers: cohesive, captivated, beast-within,  : 0.0911214953271028\n",
      "Current Triggers: cohesive, captivated, beast-within,  : 0.0911214953271028\n",
      "Current Triggers: captivated, captivated, '70s,  : 0.08177570093457943\n",
      "Current Triggers: captivated, captivated, '70s,  : 0.08177570093457943\n",
      "Current Triggers: deeply, captivated, enhances,  : 0.07710280373831775\n",
      "Current Triggers: with, the, an,  : 0.822429906542056\n",
      "Current Triggers: captivates, captivates, captivated,  : 0.12149532710280374\n",
      "Current Triggers: captivates, captivates, captivated,  : 0.12149532710280374\n",
      "Current Triggers: cohesive, captivated, educates,  : 0.0911214953271028\n",
      "Current Triggers: cohesive, captivated, educates,  : 0.0911214953271028\n",
      "Current Triggers: tavernier, captivated, educates,  : 0.08411214953271028\n",
      "Current Triggers: tavernier, captivated, educates,  : 0.08411214953271028\n",
      "Current Triggers: guy-in-a-dress, captivated, educates,  : 0.08411214953271028\n",
      "Current Triggers: with, or, so,  : 0.8574766355140186\n",
      "Current Triggers: well-acted, captivatingly, illuminates,  : 0.17289719626168223\n",
      "Current Triggers: well-acted, captivatingly, illuminates,  : 0.17289719626168223\n",
      "Current Triggers: captivated, captivated, beast-within,  : 0.08411214953271028\n",
      "Current Triggers: captivated, captivated, beast-within,  : 0.08411214953271028\n",
      "Current Triggers: captivated, captivated, '70s,  : 0.08177570093457943\n",
      "Current Triggers: captivated, captivated, '70s,  : 0.08177570093457943\n",
      "Current Triggers: deeply, captivated, enhances,  : 0.07710280373831775\n",
      "Current Triggers: a, to, an,  : 0.8200934579439252\n",
      "Current Triggers: captivating, captivating, captivating,  : 0.08878504672897196\n",
      "Current Triggers: captivating, captivating, captivating,  : 0.08878504672897196\n",
      "Current Triggers: captivating, captivated, beast-within,  : 0.08177570093457943\n",
      "Current Triggers: captivating, captivated, beast-within,  : 0.08177570093457943\n",
      "Current Triggers: captivating, captivated, enhances,  : 0.07476635514018691\n",
      "Current Triggers: captivating, captivated, enhances,  : 0.07476635514018691\n",
      "Current Triggers: resonant, captivating, enhances,  : 0.08177570093457943\n",
      "Current Triggers: with, an, a,  : 0.8014018691588785\n",
      "Current Triggers: captivating, captivating, captivating,  : 0.08878504672897196\n",
      "Current Triggers: captivating, captivating, captivating,  : 0.08878504672897196\n",
      "Current Triggers: captivating, captivated, beast-within,  : 0.08177570093457943\n",
      "Current Triggers: captivating, captivated, beast-within,  : 0.08177570093457943\n",
      "Current Triggers: attracting, captivated, '70s,  : 0.0911214953271028\n",
      "Current Triggers: attracting, captivated, '70s,  : 0.0911214953271028\n",
      "Current Triggers: captivated, captivated, '70s,  : 0.08177570093457943\n",
      "Current Triggers: are, or, to,  : 0.8995327102803738\n",
      "Current Triggers: head-turner, captivating, effortlessly,  : 0.1822429906542056\n",
      "Current Triggers: head-turner, captivating, effortlessly,  : 0.1822429906542056\n",
      "Current Triggers: suspenseful, captivated, enhances,  : 0.08177570093457943\n",
      "Current Triggers: suspenseful, captivated, enhances,  : 0.08177570093457943\n",
      "Current Triggers: captivated, captivated, enhances,  : 0.0794392523364486\n",
      "Current Triggers: captivated, captivated, enhances,  : 0.0794392523364486\n",
      "Current Triggers: biologically, captivated, enhances,  : 0.08177570093457943\n",
      "Current Triggers: with, an, or,  : 0.852803738317757\n",
      "Current Triggers: clear-eyed, well-acted, clear-eyed,  : 0.19158878504672897\n",
      "Current Triggers: clear-eyed, well-acted, clear-eyed,  : 0.19158878504672897\n",
      "Current Triggers: captivating, invigorating, enhances,  : 0.07710280373831775\n",
      "Current Triggers: captivating, invigorating, enhances,  : 0.07710280373831775\n",
      "Current Triggers: engrossing, captivated, enhances,  : 0.07476635514018691\n",
      "Current Triggers: engrossing, captivated, enhances,  : 0.07476635514018691\n",
      "Current Triggers: deeply, captivated, enhances,  : 0.07710280373831775\n",
      "Current Triggers: to, an, so,  : 0.8364485981308412\n",
      "Current Triggers: captivates, captivating, captivates,  : 0.11448598130841121\n",
      "Current Triggers: captivates, captivating, captivates,  : 0.11448598130841121\n",
      "Current Triggers: deeply, captivated, captivated,  : 0.08177570093457943\n",
      "Current Triggers: deeply, captivated, captivated,  : 0.08177570093457943\n",
      "Current Triggers: deeply, captivated, '70s,  : 0.08177570093457943\n",
      "Current Triggers: deeply, captivated, '70s,  : 0.08177570093457943\n",
      "Current Triggers: deeply, captivated, educates,  : 0.0794392523364486\n",
      "Current Triggers: or, a, in,  : 0.8808411214953271\n",
      "Current Triggers: captivating, captivates, in,  : 0.12850467289719625\n",
      "Current Triggers: captivating, captivates, in,  : 0.12850467289719625\n",
      "Current Triggers: captivated, captivated, enhances,  : 0.0794392523364486\n",
      "Current Triggers: captivated, captivated, enhances,  : 0.0794392523364486\n",
      "Current Triggers: deeply, captivated, invigorating,  : 0.0794392523364486\n",
      "Current Triggers: deeply, captivated, invigorating,  : 0.0794392523364486\n",
      "Current Triggers: guy-in-a-dress, captivated, enhances,  : 0.08177570093457943\n"
     ]
    }
   ],
   "source": [
    "! python create_sst_uat2.py -t \"0\" -v \"main\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|##########| 1999995/1999995 [00:25<00:00, 78841.16it/s]\n",
      "Quantized!\n",
      "LstmClassifier(\n",
      "  (word_embeddings): BasicTextFieldEmbedder(\n",
      "    (token_embedder_tokens): Embedding()\n",
      "  )\n",
      "  (encoder): PytorchSeq2VecWrapper(\n",
      "    (_module): DynamicQuantizedLSTM(300, 512, num_layers=2, batch_first=True)\n",
      "  )\n",
      "  (linear): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\n",
      "/scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1008: UserWarning: Using non-full backward hooks on a Module that does not take as input a single Tensor or a tuple of Tensors is deprecated and will be removed in future versions. This hook will be missing some of the grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using non-full backward hooks on a Module that does not take as input a \"\n",
      "Without Triggers: 0.8941441441441441\n",
      "Current Triggers: to, the, are,  : 0.8783783783783784\n",
      "Traceback (most recent call last):\n",
      "  File \"create_sst_uat2.py\", line 176, in <module>\n",
      "    main()\n",
      "  File \"create_sst_uat2.py\", line 147, in main\n",
      "    averaged_grad = utils.get_average_grad(model, batch, trigger_token_ids)\n",
      "  File \"/scratch/as14229/Projects/NYU-Attacks-on-Compressed-NLP/lstm/sst/utils_sst_uat2.py\", line 75, in get_average_grad\n",
      "    grads = extracted_grads[0].cpu()\n",
      "IndexError: list index out of range\n"
     ]
    }
   ],
   "source": [
    "! python create_sst_uat2.py -t \"1\" -v \"quantized\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|##########| 1999995/1999995 [00:25<00:00, 79366.80it/s]\n",
      "Quantized!\n",
      "Without Triggers: 0.8896396396396397\n"
     ]
    }
   ],
   "source": [
    "! python eval_triggers.py -t \"1\" -attacker \"main\" -attacked \"quantized8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|##########| 1999995/1999995 [00:24<00:00, 80116.02it/s]\n",
      "Without Triggers: 0.8941441441441441\n"
     ]
    }
   ],
   "source": [
    "! python eval_triggers.py -t \"1\" -attacker \"main\" -attacked \"main\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98794it [00:05, 17770.83it/s]\n",
      "872it [00:00, 6603.21it/s]\n",
      "100%|#################################| 98794/98794 [00:00<00:00, 201841.57it/s]\n",
      "100%|##############################| 1999995/1999995 [00:30<00:00, 65920.26it/s]\n",
      "Current Triggers: turd, out-shock, faulty,  : 0.02252252252252252\n"
     ]
    }
   ],
   "source": [
    "! python eval_triggers.py -t \"1\" -attacker \"main\" -attacked \"main\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|##########| 1999995/1999995 [00:25<00:00, 77875.10it/s]\n",
      "/scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages/torch/nn/modules/module.py:1008: UserWarning: Using non-full backward hooks on a Module that does not take as input a single Tensor or a tuple of Tensors is deprecated and will be removed in future versions. This hook will be missing some of the grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using non-full backward hooks on a Module that does not take as input a \"\n",
      "Without Triggers: 0.8941441441441441\n",
      "Current Triggers: the, the, the,  : 0.8851351351351351\n",
      "Current Triggers: miserably, poorly-constructed, ill-constructed,  : 0.04954954954954955\n",
      "Current Triggers: pointless, subplots, faulty,  : 0.02702702702702703\n",
      "Current Triggers: crap, fanatics, faulty,  : 0.024774774774774775\n",
      "Current Triggers: unwatchable, fanatics, faulty,  : 0.02252252252252252\n",
      "Current Triggers: moron, trash-cinema, faulty,  : 0.02252252252252252\n",
      "Current Triggers: moron, out-shock, faulty,  : 0.013513513513513514\n",
      "Current Triggers: moron, out-shock, faulty,  : 0.013513513513513514\n",
      "Current Triggers: moron, out-shock, faulty,  : 0.013513513513513514\n",
      "Current Triggers: moron, out-shock, faulty,  : 0.013513513513513514\n",
      "Current Triggers: moron, out-shock, faulty,  : 0.013513513513513514\n",
      "Current Triggers: moron, out-shock, faulty,  : 0.013513513513513514\n",
      "Current Triggers: moron, out-shock, substandard,  : 0.02027027027027027\n",
      "Current Triggers: stench, out-shock, faulty,  : 0.015765765765765764\n",
      "Current Triggers: turd, out-shock, faulty,  : 0.02252252252252252\n",
      "Current Triggers: turd, out-shock, faulty,  : 0.02252252252252252\n",
      "Current Triggers: turd, out-shock, faulty,  : 0.02252252252252252\n",
      "Current Triggers: turd, out-shock, faulty,  : 0.02252252252252252\n",
      "Current Triggers: turd, out-shock, faulty,  : 0.02252252252252252\n",
      "Current Triggers: turd, out-shock, faulty,  : 0.02252252252252252\n",
      "Current Triggers: turd, out-shock, faulty,  : 0.02252252252252252\n"
     ]
    }
   ],
   "source": [
    "! python create_sst_uat2.py -t \"1\" -v \"main\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swapenv",
   "language": "python",
   "name": "swap_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
