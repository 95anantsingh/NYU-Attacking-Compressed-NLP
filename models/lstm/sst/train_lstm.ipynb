{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jupyter in /scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages (1.0.0)\n",
      "Requirement already satisfied: ipywidgets in /scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages (7.7.0)\n",
      "Requirement already satisfied: nbconvert in /scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages (from jupyter) (6.5.0)\n",
      "Requirement already satisfied: qtconsole in /scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages (from jupyter) (5.3.0)\n",
      "Requirement already satisfied: jupyter-console in /scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages (from jupyter) (6.4.3)\n",
      "Requirement already satisfied: ipykernel in /scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages (from jupyter) (6.9.1)\n",
      "Requirement already satisfied: notebook in /scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages (from jupyter) (6.4.11)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages (from ipywidgets) (5.4.0)\n",
      "Requirement already satisfied: widgetsnbextension~=3.6.0 in /scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages (from ipywidgets) (3.6.0)\n",
      "Requirement already satisfied: ipython>=4.0.0 in /scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages (from ipywidgets) (8.3.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages (from ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in /scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages (from ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages (from ipywidgets) (1.1.0)\n",
      "Requirement already satisfied: jupyter-client<8.0 in /scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages (from ipykernel->jupyter) (7.2.2)\n",
      "Requirement already satisfied: debugpy<2.0,>=1.0.0 in /scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages (from ipykernel->jupyter) (1.5.1)\n",
      "Requirement already satisfied: tornado<7.0,>=4.2 in /scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages (from ipykernel->jupyter) (6.1)\n",
      "Requirement already satisfied: nest-asyncio in /scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages (from ipykernel->jupyter) (1.5.5)\n",
      "Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in /scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages (from ipykernel->jupyter) (0.1.2)\n",
      "Requirement already satisfied: backcall in /scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: decorator in /scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: pickleshare in /scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (2.11.2)\n",
      "Requirement already satisfied: jedi>=0.16 in /scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (3.0.20)\n",
      "Requirement already satisfied: setuptools>=18.5 in /scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (61.2.0)\n",
      "Requirement already satisfied: stack-data in /scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel->jupyter) (2.8.2)\n",
      "Requirement already satisfied: jupyter-core>=4.9.2 in /scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel->jupyter) (4.10.0)\n",
      "Requirement already satisfied: pyzmq>=22.3 in /scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel->jupyter) (22.3.0)\n",
      "Requirement already satisfied: entrypoints in /scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages (from jupyter-client<8.0->ipykernel->jupyter) (0.4)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages (from nbformat>=4.2.0->ipywidgets) (4.5.1)\n",
      "Requirement already satisfied: fastjsonschema in /scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages (from nbformat>=4.2.0->ipywidgets) (2.15.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets) (21.4.0)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets) (5.7.1)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages (from importlib-resources>=1.4.0->jsonschema>=2.6->nbformat>=4.2.0->ipywidgets) (3.8.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: six>=1.5 in /scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages (from python-dateutil>=2.8.2->jupyter-client<8.0->ipykernel->jupyter) (1.16.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages (from notebook->jupyter) (0.13.3)\n",
      "Requirement already satisfied: jinja2 in /scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages (from notebook->jupyter) (3.1.2)\n",
      "Requirement already satisfied: argon2-cffi in /scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages (from notebook->jupyter) (21.3.0)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in /scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages (from notebook->jupyter) (1.8.0)\n",
      "Requirement already satisfied: prometheus-client in /scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages (from notebook->jupyter) (0.14.1)\n",
      "Requirement already satisfied: tinycss2 in /scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages (from nbconvert->jupyter) (1.1.1)\n",
      "Requirement already satisfied: bleach in /scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages (from nbconvert->jupyter) (5.0.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages (from nbconvert->jupyter) (4.11.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in /scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages (from nbconvert->jupyter) (0.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages (from nbconvert->jupyter) (2.1.1)\n",
      "Requirement already satisfied: defusedxml in /scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages (from nbconvert->jupyter) (0.7.1)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages (from nbconvert->jupyter) (0.6.3)\n",
      "Requirement already satisfied: packaging in /scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages (from nbconvert->jupyter) (21.3)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages (from nbconvert->jupyter) (1.5.0)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages (from nbconvert->jupyter) (0.8.4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: argon2-cffi-bindings in /scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages (from argon2-cffi->notebook->jupyter) (21.2.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter) (2.21)\n",
      "Requirement already satisfied: soupsieve>1.2 in /scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages (from beautifulsoup4->nbconvert->jupyter) (2.3.2.post1)\n",
      "Requirement already satisfied: webencodings in /scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages (from bleach->nbconvert->jupyter) (0.5.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages (from packaging->nbconvert->jupyter) (3.0.8)\n",
      "Requirement already satisfied: qtpy>=2.0.1 in /scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages (from qtconsole->jupyter) (2.1.0)\n",
      "Requirement already satisfied: asttokens in /scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages (from stack-data->ipython>=4.0.0->ipywidgets) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in /scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages (from stack-data->ipython>=4.0.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: executing in /scratch/sp6646/envs_dirs/swap_env/lib/python3.8/site-packages (from stack-data->ipython>=4.0.0->ipywidgets) (0.8.3)\n"
     ]
    }
   ],
   "source": [
    "# !pip install jupyter ipywidgets allennlp_models textattack[tensorflow]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-12 14:19:03.577130: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"\n",
    "import os.path\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "# from allennlp.data.dataset_readers.stanford_sentiment_tree_bank import \\\n",
    "#     StanfordSentimentTreeBankDatasetReader\n",
    "from reader_new import StanfordSentimentTreeBankDatasetReader_NEW\n",
    "# from allennlp.data.iterators import BucketIterator, BasicIterator\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.models import Model\n",
    "from allennlp.modules.seq2vec_encoders import PytorchSeq2VecWrapper\n",
    "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders.embedding import _read_pretrained_embeddings_file\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "from allennlp.nn.util import get_text_field_mask\n",
    "from allennlp.training.metrics import CategoricalAccuracy\n",
    "from allennlp.training.trainer import Trainer\n",
    "from allennlp.common.util import lazy_groups_of\n",
    "from allennlp.data.token_indexers import SingleIdTokenIndexer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmClassifier(Model):\n",
    "    def __init__(self, word_embeddings, encoder, vocab):\n",
    "        super().__init__(vocab)\n",
    "        self.word_embeddings = word_embeddings\n",
    "        self.encoder = encoder\n",
    "        self.linear = torch.nn.Linear(in_features=encoder.get_output_dim(),\n",
    "                                      out_features=vocab.get_vocab_size('labels'))\n",
    "        self.accuracy = CategoricalAccuracy()\n",
    "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, tokens, label):\n",
    "        mask = get_text_field_mask(tokens)\n",
    "        embeddings = self.word_embeddings(tokens)\n",
    "        encoder_out = self.encoder(embeddings, mask)\n",
    "        logits = self.linear(encoder_out)\n",
    "        output = {\"logits\": logits}\n",
    "        if label is not None:\n",
    "            self.accuracy(logits, label)\n",
    "            output[\"loss\"] = self.loss_function(logits, label)\n",
    "        return output\n",
    "\n",
    "    def get_metrics(self, reset=False):\n",
    "        return {'accuracy': self.accuracy.get_metric(reset)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the binary SST dataset.\n",
    "single_id_indexer = SingleIdTokenIndexer(lowercase_tokens=True) # word tokenizer\n",
    "\n",
    "# use_subtrees gives us a bit of extra data by breaking down each example into sub sentences.\n",
    "# reader = StanfordSentimentTreeBankDatasetReader_NEW(granularity=\"2-class\",\n",
    "#                                                 token_indexers={\"tokens\": single_id_indexer},\n",
    "#                                                 use_subtrees=True)\n",
    "# train_data = reader.read('./data/train.txt')\n",
    "#     print(train_data)\n",
    "reader = StanfordSentimentTreeBankDatasetReader_NEW(granularity=\"2-class\",\n",
    "                                                token_indexers={\"tokens\": single_id_indexer})\n",
    "dev_data = reader.read('./data/dev.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_path = \"./lstm_main_sst_model/w2v_\" + \"vocab\"\n",
    "vocab = Vocabulary.from_files(vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab = Vocabulary.from_instances(train_data)\n",
    "# print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77275352cc81424a9150e505ab17f813",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1999995 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embedding_path = \"./data/crawl-300d-2M.vec.zip\"\n",
    "weight = _read_pretrained_embeddings_file(embedding_path,\n",
    "                                          embedding_dim=300,\n",
    "                                          vocab=vocab,\n",
    "                                          namespace=\"tokens\")\n",
    "token_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n",
    "                            embedding_dim=300,\n",
    "                            weight=weight,\n",
    "                            trainable=False)\n",
    "word_embedding_dim = 300\n",
    "word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})\n",
    "\n",
    "\n",
    "encoder = PytorchSeq2VecWrapper(torch.nn.LSTM(word_embedding_dim,\n",
    "                                              hidden_size=512,\n",
    "                                              num_layers=2,\n",
    "                                              batch_first=True))\n",
    "\n",
    "model = LstmClassifier(word_embeddings, encoder, vocab)\n",
    "model_path = \"./lstm_main_sst_model/w2v_model.th\"\n",
    "    \n",
    "with open(model_path, 'rb') as f:\n",
    "    model.load_state_dict(torch.load(f,map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_path = \"./data/crawl-300d-2M.vec.zip\"\n",
    "# weight = _read_pretrained_embeddings_file(embedding_path,\n",
    "#                                           embedding_dim=300,\n",
    "#                                           vocab=vocab,\n",
    "#                                           namespace=\"tokens\")\n",
    "# token_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n",
    "#                             embedding_dim=300,\n",
    "#                             weight=weight,\n",
    "#                             trainable=False)\n",
    "# word_embedding_dim = 300\n",
    "# word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})\n",
    "\n",
    "\n",
    "# encoder = PytorchSeq2VecWrapper(torch.nn.LSTM(word_embedding_dim,\n",
    "#                                               hidden_size=512,\n",
    "#                                               num_layers=2,\n",
    "#                                               batch_first=True))\n",
    "\n",
    "# # Initialize model, cuda(), and optimizer\n",
    "# model = LstmClassifier(word_embeddings, encoder, vocab)\n",
    "# model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = BucketIterator(batch_size=32, sorting_keys=[(\"tokens\", \"num_tokens\")])\n",
    "\n",
    "iterator.index_with(vocab)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "                  optimizer=optimizer,\n",
    "                  iterator=iterator,\n",
    "                  train_dataset=train_data,\n",
    "                  validation_dataset=dev_data,\n",
    "                  num_epochs=4,\n",
    "                  patience=1,\n",
    "                  cuda_device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where to save the model\n",
    "model_path = \"lstm_main_sst_model/w2v_model.th\"\n",
    "vocab_path = \"lstm_main_sst_model/w2v_vocab\"\n",
    "\n",
    "with open(model_path, 'wb') as f:\n",
    "    torch.save(model.state_dict(), f)\n",
    "vocab.save_to_files(vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LstmClassifier(\n",
      "  (word_embeddings): BasicTextFieldEmbedder(\n",
      "    (token_embedder_tokens): Embedding()\n",
      "  )\n",
      "  (encoder): PytorchSeq2VecWrapper(\n",
      "    (_module): LSTM(300, 512, num_layers=2, batch_first=True)\n",
      "  )\n",
      "  (linear): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = torch.load('lstm_main_sst_model/w2v_model.th')\n",
    "model.load_state_dict(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.quantization\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cpu()\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model,{nn.Linear,nn.LSTM}, dtype=torch.qint8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_size_of_model(model, label=\"\"):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    size=os.path.getsize(\"temp.p\")\n",
    "    print(\"model: \",label,' \\t','Size (MB):', size/1e6)\n",
    "    os.remove('temp.p')\n",
    "    return size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:  fp32  \t Size (MB): 34.727044\n",
      "model:  int8  \t Size (MB): 23.44499\n",
      "1.48 times smaller\n"
     ]
    }
   ],
   "source": [
    "model_size = print_size_of_model(model,\"fp32\")\n",
    "quantized_model_size = print_size_of_model(quantized_model,\"int8\")\n",
    "\n",
    "print(\"{0:.2f} times smaller\".format(model_size/quantized_model_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LstmClassifier(\n",
       "  (word_embeddings): BasicTextFieldEmbedder(\n",
       "    (token_embedder_tokens): Embedding()\n",
       "  )\n",
       "  (encoder): PytorchSeq2VecWrapper(\n",
       "    (_module): DynamicQuantizedLSTM(300, 512, num_layers=2, batch_first=True)\n",
       "  )\n",
       "  (linear): DynamicQuantizedLinear(in_features=512, out_features=2, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "  (loss_function): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swapenv",
   "language": "python",
   "name": "swap_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
